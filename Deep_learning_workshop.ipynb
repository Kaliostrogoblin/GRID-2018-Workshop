{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning workshop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Kaliostrogoblin/GRID-2018-Workshop/blob/master/Deep_learning_workshop.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DBCB39c2ufcC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for beginners with exciting example of computing poetry"
      ]
    },
    {
      "metadata": {
        "id": "3aGYA81BMly5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep learning libraries"
      ]
    },
    {
      "metadata": {
        "id": "-8YO6o_kp2AF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are a plenty of various frameworks for applying deep learning. Some of them you can see on the picture below."
      ]
    },
    {
      "metadata": {
        "id": "Kiznt5AEot_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Hello](https://www.analyticsindiamag.com/wp-content/uploads/2018/04/all_libraries.png =600x250)"
      ]
    },
    {
      "metadata": {
        "id": "wenu_X9eqTse",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But we will focus only on two very common libraries:\n",
        "\n",
        "\n",
        "*   [TensorFlow](https://www.tensorflow.org/)\n",
        "*   [Keras](https://keras.io/)\n",
        "\n",
        "**TensorFlow** is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.\n",
        "\n",
        "**Keras** is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. *Being able to go from idea to result with the least possible delay is key to doing good research.*\n",
        "\n",
        "**In January 2017 Keras became a part of TensorFlow library.**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AabVgx_DqCZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's see, how easy to create a deep convolutional network with the help of Keras. "
      ]
    },
    {
      "metadata": {
        "id": "wW_FBBqswXwT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will learn how to classify [handwritten digits](http://yann.lecun.com/exdb/mnist/) using deep convnet."
      ]
    },
    {
      "metadata": {
        "id": "FW624mADwpVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D, MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# specify a number of output classes\n",
        "# MNIST data has ten categories, each means particular digit\n",
        "N_CLASSES  = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63OVtfyJw28W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load and preprocess MNIST data¶\n",
        "\n",
        "Data splitted in two sets - train and test"
      ]
    },
    {
      "metadata": {
        "id": "4Ai_qqtiw6YH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "b96187d7-240c-4147-cbaf-38dbb1f76bc5"
      },
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test) = mnist.load_data() \n",
        " \n",
        "print('Train size: {:5}'.format(len(X_train)))\n",
        "print('Test size: {:6}'.format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 7s 1us/step\n",
            "Train size: 60000\n",
            "Test size:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sDoxO3E6w92v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Rescale original image data to be in range [0, 1]"
      ]
    },
    {
      "metadata": {
        "id": "1KqtB-p4w-nS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6c888713-04a0-4eba-e807-4786cb7694b5"
      },
      "cell_type": "code",
      "source": [
        "# before rescaling\n",
        "print('X_max before rescaling - {}'.format(X_train.max()))\n",
        "\n",
        "X_train_rescaled = X_train / 255.\n",
        "X_test_rescaled  = X_test / 255.\n",
        "# after rescaling\n",
        "print('X_max after rescaling  - {}'.format(X_train_rescaled.max()))\n",
        "\n",
        "# and reshape for convolutional neural network\n",
        "# each image should be 3D tensor: 28x28x1\n",
        "X_train_rescaled = X_train_rescaled[:, :, :, np.newaxis]\n",
        "X_test_rescaled = X_test_rescaled[:, :, :, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_max before rescaling - 255\n",
            "X_max after rescaling  - 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bVN1IFvUxApT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show one training example"
      ]
    },
    {
      "metadata": {
        "id": "EX5FfNVyxGDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "2cae29cf-4e25-44c0-fb46-ce598651bfe3"
      },
      "cell_type": "code",
      "source": [
        "# randomly choose index of the image to show\n",
        "i = np.random.randint(0, 9999)\n",
        "\n",
        "plt.figure(1, figsize=(2, 2))\n",
        "plt.imshow(X_test_rescaled[i].reshape((28, 28)))\n",
        "plt.title(y_test[i])\n",
        "plt.grid(False)\n",
        "plt.gray()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI0AAACbCAYAAABf0RgjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBpDQ9IYGpLG0JA0hoakMTQkjaEhaf8B39UqWepbVrAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e4dc70e48>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MIu0OfV1xcQ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Represent categorical labels as one-hot vectors\n",
        "\n",
        "For example, imagine that we have ten classes of data samples. So for the label equals to 3 one-hot vector would be:\n",
        "\n",
        "```python\n",
        "0 0 0 1 0 0 0 0 0 0\n",
        "```\n",
        "\n",
        "This transformation is nessesary for computing categorical cross-entropy loss:\n",
        "\n",
        "$$L = -\\sum_{i}^N{L_i \\log{(S_i)}}$$\n",
        "\n",
        "Where $S$ is output from Softmax Layer and $L$ is Labels"
      ]
    },
    {
      "metadata": {
        "id": "vft-6eijxF-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_categorical = to_categorical(y_train, num_classes=N_CLASSES)\n",
        "y_test_categorical  = to_categorical(y_test, num_classes=N_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWfSN7rmx2Mp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Build convolutional classifier"
      ]
    },
    {
      "metadata": {
        "id": "MZ_PmtykxF7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "472621b2-86b6-4d72-9d60-8ff07017bb3f"
      },
      "cell_type": "code",
      "source": [
        "ConvNN = Sequential()\n",
        "\n",
        "# 32 means the number of filters, features\n",
        "# (3,3) is tuple represents the filters sizes, e.g. each filter is a square with size 3x3\n",
        "# padding='same' - we add zeros across each of edges of the input sample\n",
        "# (28, 28, 1) - input shape, where 1 is a single gray channel (we may have RGB images with shape = 28,28,3)\n",
        "ConvNN.add(Conv2D(32, (5,5), input_shape=(28, 28, 1), activation='relu'))\n",
        "ConvNN.add(MaxPool2D(pool_size=(2,2)))\n",
        "ConvNN.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
        "ConvNN.add(MaxPool2D(pool_size=(2,2)))\n",
        "ConvNN.add(Flatten()) # operation that flattens input tensor\n",
        "ConvNN.add(Dense(1024, activation='relu'))\n",
        "ConvNN.add(Dense(N_CLASSES, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "ConvNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# print model\n",
        "ConvNN.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              2360320   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 2,422,666\n",
            "Trainable params: 2,422,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GTJe6tQUxF4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7ae4f7af-57ae-4b54-afd4-da3dcd2e33ae"
      },
      "cell_type": "code",
      "source": [
        "print('Training')\n",
        "ConvNN.fit(\n",
        "    X_train_rescaled, \n",
        "    y_train_categorical, \n",
        "    epochs=5, \n",
        "    validation_data=(X_test_rescaled, y_test_categorical)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 24s 407us/step - loss: 0.0962 - acc: 0.9697 - val_loss: 0.0313 - val_acc: 0.9908\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 23s 381us/step - loss: 0.0353 - acc: 0.9889 - val_loss: 0.0282 - val_acc: 0.9900\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 23s 381us/step - loss: 0.0248 - acc: 0.9922 - val_loss: 0.0307 - val_acc: 0.9905\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 23s 380us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0339 - val_acc: 0.9913\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 23s 381us/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0285 - val_acc: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e4ddb0470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "8T7vP9UPzOTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate our classifier"
      ]
    },
    {
      "metadata": {
        "id": "QIVKCgRAxF0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f303b5e4-7f99-4766-b6f2-5b880e3ea4ae"
      },
      "cell_type": "code",
      "source": [
        "#Convolutional Network\n",
        "acc = ConvNN.evaluate(X_test_rescaled, y_test_categorical)[1]\n",
        "acc *= 100\n",
        "print('ConvNN accuracy: {:.2f}%'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 114us/step\n",
            "ConvNN accuracy: 99.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9-XERJ_0C6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How it works? \n",
        "\n",
        "As you know every neural network even the deepest one is a very complex differentiable function, training of which consists of finding optimal weight values. This can be done via performing gradient descent optimization procedure, which requires computing of derivative of the loss function with respect to weights.\n",
        "\n",
        "**How to compute a derivative of such complex function?**"
      ]
    },
    {
      "metadata": {
        "id": "eGknyIrd3kv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The composite function rule\n",
        "\n",
        "If $y$ is a function of $u$ and $u$ is a function of $x$ then\n",
        "\n",
        "$$\\frac{dy}{dx}=\\frac{dy}{du} \\times \\frac{du}{dx}$$\n",
        "\n",
        "**Example**:   \n",
        "if $y = (z^3 + 4z^2 − 3z − 3)^{−6}$,\n",
        "\n",
        "then set $u = z^3 + 4z^2 − 3z − 3$ so that $y = u^{−6}$ and\n",
        "\n",
        "$$\\frac{dy}{dx}=\\frac{dy}{du} \\times \\frac{du}{dx}=−6u^{−7} × (3z^2 + 8z − 3)$$"
      ]
    },
    {
      "metadata": {
        "id": "ACCpWRSm6COO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The most convenient representation in the form of a composition is a representation in the form of a **graph of computations.** \n",
        "\n",
        "Let's see, how to represent the function \n",
        "\n",
        "$$e=(a+b)∗(b+1)$$ \n",
        "\n",
        "To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node’s value is the input to another node, an arrow goes from one to another.\n",
        "\n",
        "![computation_graph](http://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png =350x200)"
      ]
    },
    {
      "metadata": {
        "id": "uZLD2PMj9IwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Derivatives on Computational Graphs\n",
        "\n",
        "If one wants to understand derivatives in a computational graph, the key is to understand derivatives on the edges.\n",
        "\n",
        "![derivatives_on_computation_graph](http://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png =450x290)\n",
        "\n",
        "And to get the derivative of e with respect to b we obtain:\n",
        "\n",
        "$$e=(a+b)∗(b+1), \\frac{de}{db}=1∗2+1∗3$$"
      ]
    },
    {
      "metadata": {
        "id": "gIzphMtk-1dt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**To explore this question read a [blog-post](http://colah.github.io/posts/2015-08-Backprop/) about \"Calculus on Computational Graphs\".**"
      ]
    },
    {
      "metadata": {
        "id": "0WIHWXgg_rex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Language Modeling or how to train a neural network writing lyrics\n",
        "\n",
        "**Note: this example is retrieved from very exciting [russian book](https://www.litres.ru/a-kadurin-13464223/glubokoe-obuchenie-pogruzhenie-v-mir-neyronnyh-29817855/) about deep learning.**"
      ]
    },
    {
      "metadata": {
        "id": "NKAt9ZFsF7-c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Target function in such kind of problems is to predict how a text will go on. \n",
        "\n",
        "The idea is very simple: **let's try to generate the text letter by letter** treating it simply as a stream of symbols.\n",
        "\n",
        "Then the task of language modeling is very easily formalized: **we would like to predict the next character from the previous text.**\n",
        "\n",
        "We will use a recurrent neural network as a natural way to work with sequential data. The input is a cutted sentence and the output is the next character in this sentence. So we need to solve the **classification problem with multiple classes.**"
      ]
    },
    {
      "metadata": {
        "id": "O-p4pH5zHegC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Clone the repository with training data"
      ]
    },
    {
      "metadata": {
        "id": "FYv6ncn9upEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c7a85382-780d-477f-da28-e662f3ee9cf7"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Kaliostrogoblin/GRID-2018-Workshop.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GRID-2018-Workshop'...\n",
            "remote: Counting objects: 13, done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 13 (delta 1), reused 4 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7XJTUUi2HrcH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import some libraries and define constants."
      ]
    },
    {
      "metadata": {
        "id": "nRmpZ0wwvSYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fc60ec27-7778-4969-d198-66563802564f"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# sentence tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# for pretty formatted output\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "input_fname = 'GRID-2018-Workshop/data/evgeniy_onegin.txt'\n",
        "output_fname = 'sampling.log'\n",
        "\n",
        "RANDOM_STATE = 13"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JhVg1iFDH8sy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a dictionary of characters\n",
        "\n",
        "The problem is to predict the next character with respect to the input, but a neural networks don't understand a textual data. We have to translate each character into integer identifier. For convenience we will use a dictionary of characters."
      ]
    },
    {
      "metadata": {
        "id": "wbu2L9pKvrEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1327
        },
        "outputId": "247dedc7-1477-4978-ce18-5a4e2069eee5"
      },
      "cell_type": "code",
      "source": [
        "START_CHAR = '\\b'\n",
        "END_CHAR = '\\t'\n",
        "PADDING_CHAR = '\\a'\n",
        "\n",
        "chars = set([START_CHAR, '\\n', END_CHAR])\n",
        "print(\"Initial set of chars:\", chars)\n",
        "\n",
        "# read file for getting input chars\n",
        "print(\"Getting chars set...\")\n",
        "with open(input_fname) as f:\n",
        "    for line in f:\n",
        "        # add new chars to the set\n",
        "        chars.update(list(line.strip().lower()))\n",
        "\n",
        "print(\"Creating dictionary of chars...\")\n",
        "char_indices = {c : i for i, c in enumerate(sorted(list(chars)))}\n",
        "# add specific `PADDING CHAR` \n",
        "char_indices[PADDING_CHAR] = 0\n",
        "indices_to_chars = {i : c for c, i in char_indices.items()}\n",
        "num_chars = len(chars)\n",
        "print(\"Size of the chars dictionary:\", num_chars)\n",
        "print(\"Dictionary of chars:\")\n",
        "pprint(char_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial set of chars: {'\\x08', '\\t', '\\n'}\n",
            "Getting chars set...\n",
            "Creating dictionary of chars...\n",
            "Size of the chars dictionary: 72\n",
            "Dictionary of chars:\n",
            "{'\\x07': 0,\n",
            " '\\x08': 0,\n",
            " '\\t': 1,\n",
            " '\\n': 2,\n",
            " ' ': 3,\n",
            " '!': 4,\n",
            " '\"': 5,\n",
            " \"'\": 6,\n",
            " '(': 7,\n",
            " ')': 8,\n",
            " ',': 9,\n",
            " '-': 10,\n",
            " '.': 11,\n",
            " ':': 12,\n",
            " ';': 13,\n",
            " '?': 14,\n",
            " 'a': 15,\n",
            " 'b': 16,\n",
            " 'c': 17,\n",
            " 'd': 18,\n",
            " 'e': 19,\n",
            " 'f': 20,\n",
            " 'g': 21,\n",
            " 'h': 22,\n",
            " 'i': 23,\n",
            " 'k': 24,\n",
            " 'l': 25,\n",
            " 'm': 26,\n",
            " 'n': 27,\n",
            " 'o': 28,\n",
            " 'p': 29,\n",
            " 'q': 30,\n",
            " 'r': 31,\n",
            " 's': 32,\n",
            " 't': 33,\n",
            " 'u': 34,\n",
            " 'v': 35,\n",
            " 'w': 36,\n",
            " 'x': 37,\n",
            " 'y': 38,\n",
            " 'z': 39,\n",
            " 'а': 40,\n",
            " 'б': 41,\n",
            " 'в': 42,\n",
            " 'г': 43,\n",
            " 'д': 44,\n",
            " 'е': 45,\n",
            " 'ж': 46,\n",
            " 'з': 47,\n",
            " 'и': 48,\n",
            " 'й': 49,\n",
            " 'к': 50,\n",
            " 'л': 51,\n",
            " 'м': 52,\n",
            " 'н': 53,\n",
            " 'о': 54,\n",
            " 'п': 55,\n",
            " 'р': 56,\n",
            " 'с': 57,\n",
            " 'т': 58,\n",
            " 'у': 59,\n",
            " 'ф': 60,\n",
            " 'х': 61,\n",
            " 'ц': 62,\n",
            " 'ч': 63,\n",
            " 'ш': 64,\n",
            " 'щ': 65,\n",
            " 'ъ': 66,\n",
            " 'ы': 67,\n",
            " 'ь': 68,\n",
            " 'э': 69,\n",
            " 'ю': 70,\n",
            " 'я': 71}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YguGuapDI-68",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As for MNIST example we need to represent each character's identifier as one-hot vector for classification."
      ]
    },
    {
      "metadata": {
        "id": "x1Ex_Ttgv4ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b46d2e6c-324e-43f7-a448-7c2511616ee5"
      },
      "cell_type": "code",
      "source": [
        "# one_hot vectors creator\n",
        "def get_one(i, sz):\n",
        "    res = np.zeros(sz)\n",
        "    res[i] = 1\n",
        "    return res\n",
        "\n",
        "# one_hot vectors for each char\n",
        "print('Chars one-hot dict...')\n",
        "char_vectors = {\n",
        "    c : (\n",
        "            np.zeros(num_chars) if c == PADDING_CHAR\n",
        "            else get_one(v, num_chars)\n",
        "        ) for c, v in char_indices.items()\n",
        "}\n",
        "print('One-hot represantation of the `ф` letter:', char_vectors['ф'], '\\n')\n",
        "\n",
        "# Note, that padding char is a vector of zeros!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chars one-hot dict...\n",
            "One-hot represantation of the `ф` letter: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jGZVd5cbJaTA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read the text as list of sentences\n",
        "\n",
        "**Note:** that we are reading file twice, because the data has not a very big size, but in real conditions it may be time-consuming."
      ]
    },
    {
      "metadata": {
        "id": "Cxc2lN8Ev9Hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b3874480-4db6-4e53-bff4-cbc5467fc315"
      },
      "cell_type": "code",
      "source": [
        "# read file for getting sentences\n",
        "with open(input_fname, 'r') as f:\n",
        "    sentences = sent_tokenize(f.read().lower())\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "print(\"10-sentence: `%s`\" % sentences[10], '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 1489\n",
            "10-sentence: `iii\n",
            "\n",
            "служив отлично благородно,\n",
            "долгами жил его отец,\n",
            "давал три бала ежегодно\n",
            "и промотался наконец.` \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ww6lCh4uKN3o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing dataset\n",
        "\n",
        "The next step is vectorization. Let's define a function that turns a set of sentences into two tensors: $X$ contains the vectors of the symbols, and $y$ is the result that we need to predict. In fact, it is the same tensor $X$, only shifted one vector to the right. At time $t$ we predict a symbol that will stand in place $t + 1$."
      ]
    },
    {
      "metadata": {
        "id": "g1KyvP-7wIgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_matrices(sentences):\n",
        "    max_sentence_len = np.max([len(x) for x in sentences])\n",
        "    X = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), max_sentence_len, len(chars)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        char_seq = (START_CHAR + sentence + END_CHAR).ljust(\n",
        "            max_sentence_len+1, PADDING_CHAR)\n",
        "        \n",
        "        for t in range(max_sentence_len):\n",
        "            X[i, t, :] = char_vectors[char_seq[t]]\n",
        "            y[i, t, :] = char_vectors[char_seq[t+1]]\n",
        "            \n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQipgk9awiIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "416b0c27-4cc4-4012-a9d9-9114708eec9f"
      },
      "cell_type": "code",
      "source": [
        "print('Preparing dataset...')\n",
        "rs = np.random.RandomState(RANDOM_STATE)\n",
        "test_indices = rs.choice(len(sentences), int(len(sentences) * 0.05))\n",
        "sentences_train = [sentences[x]\n",
        "    for x in set(range(len(sentences))) - set(test_indices)]\n",
        "sentences_test = [sentences[x] for x in test_indices]\n",
        "sentences_train = sorted(sentences_train, key=lambda x: len(x))\n",
        "X_test, y_test = get_matrices(sentences_test)\n",
        "batch_size = 4\n",
        "print('N.o. train sentences:', len(sentences_train))\n",
        "print('Test shape:', X_test.shape, y_test.shape, '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset...\n",
            "N.o. train sentences: 1416\n",
            "Test shape: (74, 378, 72) (74, 378, 72) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPVHuqQiLXgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We didn't vectorize the train data to not store the whole matrix in memory, but we can create a generator, that will vectorize data in portions. Keras has a special method for training with the help of generators  --  ```fit_generator()```"
      ]
    },
    {
      "metadata": {
        "id": "JYakws5vwrSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# batch generator for model training\n",
        "def generate_batch(data, batch_size):\n",
        "    while True:\n",
        "        for i in range(len(data) // batch_size):\n",
        "            data_batch = data[i*batch_size : (i+1)*batch_size]\n",
        "            yield get_matrices(data_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dgohpn1XL-3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Keras model"
      ]
    },
    {
      "metadata": {
        "id": "ZH6KbwfPM1Kt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Do some imports."
      ]
    },
    {
      "metadata": {
        "id": "ybQfSpJNw_ef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# keras model\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Input\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTBj9XHCtIB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducible results\n",
        "\n",
        "There is a great [blog-post](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/) about neccessarity of getting reproducible results in experiments and how to obtain this reproducibility in Keras.\n",
        "\n",
        "**Note:** when I was writing this notebook, I forgot about reproducibility. Don't do that!"
      ]
    },
    {
      "metadata": {
        "id": "cIRgIPCxs6zG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(RANDOM_STATE)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(RANDOM_STATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgMpoMx1NbyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Schematic figure of the model:\n",
        "\n",
        "![language_model](https://wiki.ubc.ca/images/a/ac/CharLevel.png)"
      ]
    },
    {
      "metadata": {
        "id": "IJsR4iYkxBUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "5d79364c-c986-4ee0-981e-ca87aa308d97"
      },
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(None, num_chars))\n",
        "l1 = GRU(100, activation='tanh', return_sequences=True)(inputs)\n",
        "l1_d = Dropout(0.3)(l1)\n",
        "# skip connection\n",
        "inputs_2 = Concatenate()([inputs, l1_d])\n",
        "l2 = GRU(100, activation='tanh', return_sequences=True)(inputs_2)\n",
        "l2_d = Dropout(0.3)(l2)\n",
        "#concat all outputs\n",
        "input_d = Concatenate()([l1_d, l2_d])\n",
        "outputs = TimeDistributed(Dense(num_chars, activation='softmax'))(input_d)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, None, 72)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru_3 (GRU)                     (None, None, 100)    51900       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 100)    0           gru_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, 172)    0           input_2[0][0]                    \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_4 (GRU)                     (None, None, 100)    81900       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, None, 100)    0           gru_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, None, 200)    0           dropout_3[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 72)     14472       concatenate_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 148,272\n",
            "Trainable params: 148,272\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QefVDY2YOQ4z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, when the model is built we have to compile it by defining an optimizer, loss function and evaluation metrics. **Gradients should always be clipped during training of LSTM or GRU, particularly with deep ones.**"
      ]
    },
    {
      "metadata": {
        "id": "mMEh3FsXxZh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compiling model\n",
        "# we have to clip gradients in RNN network!\n",
        "from keras.optimizers import Adam\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=Adam(clipnorm=1.), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2u1EUDMYPEmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Callbacks\n",
        "\n",
        "A callback is a set of functions to be applied at given stages of the training procedure. One can use callbacks to get a view on internal states and statistics of the model during training or for saving logs. "
      ]
    },
    {
      "metadata": {
        "id": "XzDgZXjrxj58",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# char sampling callback\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import CSVLogger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzVBRLFtPeSn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Callback for text sampling\n",
        "\n",
        "At the output of its last layer, the model generates weights $x_w$ of characters $w$. These weights do not necessarily sum to one and do not necessarily have a sense of probability. The softmax activation function suggests that it is reasonable to choose the probabilities as follows:\n",
        "\n",
        "$$p(w)\\propto e^{-\\frac{1}{T}x_w},$$\n",
        "\n",
        "where $T$ is a **sampling temperature**.\n",
        "\n",
        "**If $T$ is large**, then the exponents will be small enough in modulus, the results of the exponentiation will not be too large, and the probabilities will be fairly close at the output, that is, **we will sample quite randomly**.\n",
        "\n",
        "And **if $T$ is small**, then exponents will be large and after exponentiation very small numbers close to zero will be obtained very often. **There will be only few non-zero probabilities. **\n",
        "\n",
        "**Example:**\n",
        "\n",
        "\n",
        "```x = [1, 2, 5], T=10.0 =>```  $p\\approx (0.39, 0.36, 0.25);$\n",
        "\n",
        "```x = [1, 2, 5], T=1.0 =>```  $p\\approx (0.72, 0.27, 0.01);$\n",
        "\n",
        "```x = [1, 2, 5], T=10.0 =>```  $p\\approx (0.9999, 4 \\cdot 10^{-5}, 4 \\cdot 10^{-18});$"
      ]
    },
    {
      "metadata": {
        "id": "tbD6C_m7xktn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharSampler(Callback):\n",
        "    def __init__(self, char_vectors, model):\n",
        "        self.char_vectors = char_vectors\n",
        "        self.model = model\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.epoch = 0\n",
        "        if os.path.isfile(output_fname):\n",
        "            os.remove(output_fname)\n",
        "\n",
        "    def sample(self, preds, temperature=0.3):\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        return np.argmax(probas)\n",
        "\n",
        "    def sample_one(self, T):\n",
        "        result = START_CHAR\n",
        "        while len(result) < 500:\n",
        "            Xsampled = np.zeros((1, len(result), num_chars))\n",
        "            \n",
        "            for t, c in enumerate(list(result)):\n",
        "                Xsampled[0, t, :] = self.char_vectors[c]\n",
        "\n",
        "            ysampled = self.model.predict(Xsampled, batch_size=1)[0, :]\n",
        "            yv = ysampled[len(result)-1, :]\n",
        "            selected_char = indices_to_chars[self.sample(yv, T)]\n",
        "            if selected_char == END_CHAR:\n",
        "                break\n",
        "            result = ''.join([result, selected_char])\n",
        "        return result\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.epoch += 1\n",
        "        if self.epoch % 10 == 0:\n",
        "            print(\"\\nEpoch %d text sampling:\" % self.epoch)\n",
        "            with open(output_fname, 'a') as outf:\n",
        "                outf.write(\"\\n===== Epoch % d =====\\n\" % self.epoch)\n",
        "                for T in [0.3, 0.5, 0.7, 1.0]:\n",
        "                    print(\"\\tsampling, T = %.1f...\" % T)\n",
        "                    for _ in range(5):\n",
        "                        self.model.reset_states()\n",
        "                        res = self.sample_one(T)\n",
        "                        outf.write(\"\\nT = %.1f\\n%s\\n\" % (T, res[1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKdS-6QZxqXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cb_sampler = CharSampler(char_vectors, model)\n",
        "cb_logger = CSVLogger(output_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxzzhwGaVltI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "XbxFgQyux52k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2315
        },
        "outputId": "99a1d66f-19d5-4336-ab7d-8d4268f967cf"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generate_batch(sentences_train, batch_size), \n",
        "    steps_per_epoch=int(len(sentences_train) / batch_size) * batch_size,\n",
        "    epochs=200,\n",
        "    verbose=True,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[cb_logger, cb_sampler]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1416/1416 [==============================] - 550s 388ms/step - loss: 1.4420 - acc: 0.5576 - val_loss: 0.4066 - val_acc: 0.1301\n",
            "Epoch 2/200\n",
            "1416/1416 [==============================] - 553s 391ms/step - loss: 1.4293 - acc: 0.5619 - val_loss: 0.4138 - val_acc: 0.1287\n",
            "Epoch 3/200\n",
            "1416/1416 [==============================] - 555s 392ms/step - loss: 1.4214 - acc: 0.5630 - val_loss: 0.4190 - val_acc: 0.1273\n",
            "Epoch 4/200\n",
            "1416/1416 [==============================] - 548s 387ms/step - loss: 1.4158 - acc: 0.5648 - val_loss: 0.4233 - val_acc: 0.1259\n",
            "Epoch 5/200\n",
            "1416/1416 [==============================] - 546s 385ms/step - loss: 1.4101 - acc: 0.5665 - val_loss: 0.4270 - val_acc: 0.1247\n",
            "Epoch 6/200\n",
            "1416/1416 [==============================] - 545s 385ms/step - loss: 1.4071 - acc: 0.5668 - val_loss: 0.4306 - val_acc: 0.1242\n",
            "Epoch 7/200\n",
            "1416/1416 [==============================] - 549s 388ms/step - loss: 1.4054 - acc: 0.5675 - val_loss: 0.4352 - val_acc: 0.1227\n",
            "Epoch 8/200\n",
            "1416/1416 [==============================] - 542s 383ms/step - loss: 1.3962 - acc: 0.5682 - val_loss: 0.4360 - val_acc: 0.1228\n",
            "Epoch 9/200\n",
            "1416/1416 [==============================] - 546s 386ms/step - loss: 1.3923 - acc: 0.5717 - val_loss: 0.4386 - val_acc: 0.1228\n",
            "Epoch 10/200\n",
            "1416/1416 [==============================] - 549s 387ms/step - loss: 1.3932 - acc: 0.5708 - val_loss: 0.4425 - val_acc: 0.1205\n",
            "\n",
            "Epoch 10 text sampling:\n",
            "\tsampling, T = 0.3...\n",
            "\tsampling, T = 0.5...\n",
            "\tsampling, T = 0.7...\n",
            "\tsampling, T = 1.0...\n",
            "Epoch 11/200\n",
            "1416/1416 [==============================] - 556s 393ms/step - loss: 1.3915 - acc: 0.5721 - val_loss: 0.4466 - val_acc: 0.1209\n",
            "Epoch 12/200\n",
            "1416/1416 [==============================] - 550s 389ms/step - loss: 1.3866 - acc: 0.5730 - val_loss: 0.4456 - val_acc: 0.1205\n",
            "Epoch 13/200\n",
            "1416/1416 [==============================] - 552s 390ms/step - loss: 1.3828 - acc: 0.5729 - val_loss: 0.4503 - val_acc: 0.1179\n",
            "Epoch 14/200\n",
            "1416/1416 [==============================] - 556s 393ms/step - loss: 1.3783 - acc: 0.5749 - val_loss: 0.4496 - val_acc: 0.1189\n",
            "Epoch 15/200\n",
            "1416/1416 [==============================] - 559s 395ms/step - loss: 1.3737 - acc: 0.5758 - val_loss: 0.4522 - val_acc: 0.1197\n",
            "Epoch 16/200\n",
            "1416/1416 [==============================] - 559s 395ms/step - loss: 1.3736 - acc: 0.5758 - val_loss: 0.4530 - val_acc: 0.1186\n",
            "Epoch 17/200\n",
            "1416/1416 [==============================] - 557s 393ms/step - loss: 1.3696 - acc: 0.5768 - val_loss: 0.4544 - val_acc: 0.1183\n",
            "Epoch 18/200\n",
            "1416/1416 [==============================] - 555s 392ms/step - loss: 1.3680 - acc: 0.5775 - val_loss: 0.4555 - val_acc: 0.1191\n",
            "Epoch 19/200\n",
            "1416/1416 [==============================] - 556s 393ms/step - loss: 1.3669 - acc: 0.5777 - val_loss: 0.4569 - val_acc: 0.1190\n",
            "Epoch 20/200\n",
            "1416/1416 [==============================] - 555s 392ms/step - loss: 1.3623 - acc: 0.5790 - val_loss: 0.4587 - val_acc: 0.1178\n",
            "\n",
            "Epoch 20 text sampling:\n",
            "\tsampling, T = 0.3...\n",
            "\tsampling, T = 0.5...\n",
            "\tsampling, T = 0.7...\n",
            "\tsampling, T = 1.0...\n",
            "Epoch 21/200\n",
            " 325/1416 [=====>........................] - ETA: 5:39 - loss: 1.3296 - acc: 0.5895"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5871182e23a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_sampler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "LV7yLvjMVy1x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sampling results"
      ]
    },
    {
      "metadata": {
        "id": "5VxRC8H8VIPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6266
        },
        "outputId": "1d473a02-f848-4562-fa64-7e8dc9971689"
      },
      "cell_type": "code",
      "source": [
        "!cat sampling.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch  10 =====\n",
            "\n",
            "T = 0.3\n",
            "как мне подумала с своей.\n",
            "\n",
            "T = 0.3\n",
            "xxxviii\n",
            "\n",
            "привечный тай разделья старой\n",
            "и стром в сердце в самом порой,\n",
            "и все своей сердце венец,\n",
            "не постели по дворанье,\n",
            "и в сердце прикрасной не полет,\n",
            "и с ней на скоро не старина,\n",
            "не полной достойный без глас,\n",
            "и с постихо уж он меня летой\n",
            "и все столи простовой строманный,\n",
            "не полной достих и следит странный,\n",
            "все не в полной достой своей,\n",
            "и взор наш его в сосед света\n",
            "пред ней светремение славы\n",
            "не полною долгой только,\n",
            "в сердце привезденье на света,\n",
            "и с нет на при соловить и след.\n",
            "\n",
            "T = 0.3\n",
            "xxxviii\n",
            "\n",
            "все дево владительный расно\n",
            "и волненье старины красе\n",
            "к моей мертвенных молесть мог,\n",
            "и все стороне, слез обратя,\n",
            "с семиной света в самом доме;\n",
            "и все столка за нем она.\n",
            "\n",
            "T = 0.3\n",
            "все тихо.\n",
            "\n",
            "T = 0.3\n",
            "xxxviii..\n",
            "\n",
            "T = 0.5\n",
            "довольно.\n",
            "\n",
            "T = 0.5\n",
            "где вы?\".\n",
            "\n",
            "T = 0.5\n",
            "ужели он?..\".. - \"я не могодь\n",
            "и пора за велишь в немой\n",
            "в столах молковать одна свой,\n",
            "на простой не грустно уж ранный,\n",
            "и слез в предуков угордая строй\n",
            "погла с поли приновал,\n",
            "старый полной корнить он порой\n",
            "славить не богла оживленья\n",
            "по весет онегина глядит.\n",
            "\n",
            "T = 0.5\n",
            "xvi.\n",
            "\n",
            "о мог он изменный тредной\n",
            "и как и свой себя в тимой тем\n",
            "вленегий здоровые дали\n",
            "и много в достого себе\n",
            "на свои погладительный,\n",
            "на ветреный тей лет и странный,\n",
            "он по веселом она всех,\n",
            "не посла записать розилась,\n",
            "в сердце танит в востововить\n",
            "смертвины на поли другий,\n",
            "и вольново перелесть нада\n",
            "страх тожко встредитвих подушать\n",
            "в тени вам карких страсти\n",
            "после не трепетной меня бесел\n",
            "с образина согружный,\n",
            "под нее молчаливая ней\n",
            "почудит возмо не постила;\n",
            "еще на вечер на страха,\n",
            "и все семья с боль\n",
            "\n",
            "T = 0.5\n",
            "- так, видно нем полей сердца,\n",
            "огрозной простотовся достой,\n",
            "как за подлядить она в ним,\n",
            "слеза на крикой до красной\n",
            "свет оставить рассудь ее свете\n",
            "и после в нем поедет в нем\n",
            "позвал и звонный издалась и,\n",
            "когда бы записти и поле,\n",
            "как с нем обезоков нашего,\n",
            "за в точь, изгранный без ей,\n",
            "под ним и застоты в весковной\n",
            "и то света с поступленный,\n",
            "и забыт не вотродой полет\n",
            "и перед шимой семяя\n",
            "страдость возволные стира\n",
            "в таком онегин постеле,\n",
            "в тесень разгаворельный нас,\n",
            "что нам он сердце в рокой поле,\n",
            "на\n",
            "\n",
            "T = 0.7\n",
            "не довето, быв еду с нема\n",
            "к песен их окросинью вастранных\n",
            "он подуманной пустойный,\n",
            "не там он важный полной поли\n",
            "соседувать моей кровь\n",
            "как сосдала она завыда;\n",
            "порой онегина в жела\n",
            "он был ваш пронаслися ноодый,\n",
            "изневих невольных позорежал;\n",
            "то все устохранил гости,\n",
            "не он сталы пред тревелот;\n",
            "в старины с карку без вереблога\n",
            "пере времлен попла меня...\n",
            "к томней, не жарать судьба без блествой,\n",
            "и жизнь его к любовной дверь,\n",
            "и после в ее мечинье,\n",
            "в нет на большую следовый зевы,\n",
            "позирает стручены...\n",
            "вста\n",
            "\n",
            "T = 0.7\n",
            "увы!\n",
            "\n",
            "T = 0.7\n",
            "вот ближе!\"\n",
            "\n",
            "T = 0.7\n",
            "но вы, что ж танете старонный,\n",
            "я строен стреблюшу татьяна,\n",
            "но монет в поли причего,\n",
            "татьяна был постихаль,\n",
            "что наш верной вдруг нас друзья,\n",
            "да в рокою прыды навыдел\n",
            "давно у треного раздовнем:\n",
            "нейдет и трудов отну просут.\n",
            "\n",
            "T = 0.7\n",
            "не шла\".... вот постели бране!\n",
            "\n",
            "T = 1.0\n",
            "но кто не сностель и крестоты\n",
            "капкайну бегости свое,\n",
            "ждом жалитв устрамернух;\n",
            "про слованний со вьсе двой он.\n",
            "\n",
            "T = 1.0\n",
            "мечтам своярованья, привачки,\n",
            "он одно и рассывена!\"\n",
            "\n",
            "T = 1.0\n",
            "- у тами\n",
            "придознавить всех опоростил,\n",
            "с имтенье мязного танем.\n",
            "\n",
            "T = 1.0\n",
            "пора та глона гнявился,\n",
            "коматаньем и лукавый\n",
            "слышит, клично сосстизиным расcец\n",
            "умолу у летей вородной,\n",
            "и беспустые жизнь моя,\n",
            "казим, ри веселый зрать\n",
            "(нак набрезник сон целый мне,\n",
            "татьяна ее поэточны,\n",
            "как давно вымили водражанный,\n",
            "глаземком настой не леньктя\n",
            "соследказа ограленый;\n",
            "доэтке с русок увысли)\n",
            "вузок\n",
            "вас остроты тет как двирил;\n",
            "услынный кличты углад роон.\n",
            "\n",
            "T = 1.0\n",
            "не, ты не причемобного скал...\n",
            "они следую пророва...\n",
            "в таком умиц и боющий,\n",
            "в кранивой расстойний жениском)\n",
            "он светь отицо в тишу; нем вередь;\n",
            "в деля, что с ней онегин был,\n",
            "и нашепу, чтенные просвяж,\n",
            "кревнепеянных скрагимый,\n",
            "он на своянной ногразплочти,\n",
            "ведав раздарасльстая долгиот\n",
            "убужуки пути моей\n",
            "давулод ар менк... кутина\n",
            "на пеньями расшимся всють,\n",
            "порам позвовленным онегин.\n",
            "\n",
            "===== Epoch  20 =====\n",
            "\n",
            "T = 0.3\n",
            "он весне полно постойный двой\n",
            "с поли больше старины,\n",
            "и волновала и пред ней\n",
            "как и сердце в нем оченье\n",
            "и все столи свете не поэта,\n",
            "не смертной молчаливой он в нем,\n",
            "подолным сердце молодой,\n",
            "воспойди на виновенный,\n",
            "и странный сон свои не глас,\n",
            "и после в ней не скаль остать,\n",
            "и полновов наши детей\n",
            "подомный славы не скальный,\n",
            "на стол не полеты полнана\n",
            "и все другой сердце в черем,\n",
            "в отмятельной довольных след,\n",
            "и в ней и не света не своей\n",
            "и все серденный грустной,\n",
            "и даль простолился на беда\n",
            "в передумен\n",
            "\n",
            "T = 0.3\n",
            "но толже!\"\n",
            "\n",
            "T = 0.3\n",
            "xxxviii\n",
            "\n",
            "в сем бы так он сердце в ней странный,\n",
            "и столь моей краса в ней.\n",
            "\n",
            "T = 0.3\n",
            "- \"я вы, постой, но забыл стревной\n",
            "и вдруг был нам в суровый день\n",
            "и в собна с нею потом,\n",
            "на только возмолны и света,\n",
            "к сердце в нем и рассковорей,\n",
            "и то мне в глубом и сердца ль,\n",
            "не посла не воскорей нас,\n",
            "полет он скажет он одной\n",
            "на поднел порадень моей\n",
            "востольно в постеле настал.\n",
            "\n",
            "T = 0.3\n",
            "не стой,\n",
            "пошел, а наше вени соне\n",
            "и все двора летит он и нас.\n",
            "\n",
            "T = 0.5\n",
            "но полно.\n",
            "\n",
            "T = 0.5\n",
            "xxxviii..\n",
            "\n",
            "T = 0.5\n",
            "он пестрей, в думой издал он\n",
            "в гостянах подушин громно:\n",
            "в онегин по вереных безелесь,\n",
            "и толковает стало чуты,\n",
            "все для не ответался он.\n",
            "\n",
            "T = 0.5\n",
            "и, помно, в кого москво,\n",
            "и влетение трепетанье,\n",
            "и с онегина мужет,\n",
            "он печальный гордой старины,\n",
            "полности не замертая,\n",
            "здустя был и с поглавой,\n",
            "страспить и душе предеть своей,\n",
            "в страхом дали на светенить,\n",
            "и попрос он старинный глед...\n",
            "\n",
            "xliii\n",
            "\n",
            "вовот он летела и в сани\n",
            "братели провыдком ветер,\n",
            "и темной луна с ней в сосед;\n",
            "подом он свето не замечает,\n",
            "в вери тречный сед был доворов,\n",
            "был под ним светеной постучки\n",
            "и крастила гробовалась,\n",
            "да тревоти старина, слежал,\n",
            "и в собранных оставлетель;\n",
            "на кров, \n",
            "\n",
            "T = 0.5\n",
            "- с какой онегина моле\n",
            "в подумить подвел и в сердце много\n",
            "на своим все клонятся вланья,\n",
            "в самок на весных летайтенья,\n",
            "все верем постучка и том,\n",
            "в после так барит и то мнов.\n",
            "\n",
            "T = 0.7\n",
            "прикомнов девочки замет;\n",
            "его писно подумал я:\n",
            "в камки силу суд будно в;\n",
            "еще не в тихо гостей,\n",
            "но полна двя сепел каменей\n",
            "и темный роздость орего,\n",
            "при сладу, пот погробой сталим;\n",
            "он страшный страста но сталенье;\n",
            "по-либовся придете погром,\n",
            "ны кровели как я не корна;\n",
            "там модит, тено неседной\n",
            "советнико модного влада,\n",
            "у головой сон котроки,\n",
            "похолои звезда провидал;\n",
            "еще не верной радо много,\n",
            "опершевом и на возкратись,\n",
            "и слезах обравор точь страненье\n",
            "и вподали, занесть уразы,\n",
            "ни многразний со дружит\n",
            "и\n",
            "\n",
            "T = 0.7\n",
            "xxxvi\n",
            "\n",
            "и кольца, по другу старить.\n",
            "\n",
            "T = 0.7\n",
            "не правда ль?\n",
            "\n",
            "T = 0.7\n",
            "xxxviii..\".\n",
            "\n",
            "T = 0.7\n",
            "не то, того, все постойлен,\n",
            "сердечно у никогоном,\n",
            "все в облек и странном согат,\n",
            "что то, как он и среди них,\n",
            "куда печальный резненный ленский,\n",
            "невольно после те невой,\n",
            "водом одно тебя,\n",
            "здесь вы признаслей кланяли\n",
            "и в блажен, так без ей себя,\n",
            "был летси старины которы,\n",
            "капал, фольна водами,\n",
            "и лежный гороот откры пройкой,\n",
            "крастослевье не вздохнул;\n",
            "о татьяна полною, читал.\n",
            "\n",
            "T = 1.0\n",
            "к ней!...\", кузаняму жатыток,\n",
            "надомнуе допонушать,\n",
            "все без, шепет другей грусть,\n",
            "и татьяну шул часе нели.\n",
            "\n",
            "T = 1.0\n",
            "все тихо.\n",
            "\n",
            "T = 1.0\n",
            "-\n",
            "\"подыв, кудны быль истиснула,\n",
            "взор любовлица ждет гробиж:\n",
            "евгений, - это прошно, ну невольный,\n",
            "чтон валский средь мирали цар\n",
            "всязок давало гости, что встрака\n",
            "влиценья након, смучить,\n",
            "водает капать, счадственку;\n",
            "утрого предсегда близны\n",
            "он?..\n",
            "\n",
            "T = 1.0\n",
            "xxxvi.i.... сять сонелись,\n",
            "как изъевою сладдой душой.\n",
            "\n",
            "T = 1.0\n",
            "не все ленских даркои мою\n",
            "и с бождет касла евай;\n",
            "мои полнаненьямивлялю\n",
            "я в него деле; поклонной, гдные го),\n",
            "мы старом встречен комной полкой\n",
            "возменили страдных роман,\n",
            "стети невинского с них пугаи...\n",
            "шумчит и жалобелонной\n",
            "ивор, кличит и сулобна гоней,\n",
            "завсе роды нам лице пеньа\n",
            "не молчу красе не укнывном\n",
            "вслмахали старимевство покнеже.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dbmFf9JDV3-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save and upload model to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "WSLZOUjIV7xj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's simply save the model. **Note that we are saving model structure with weights and optimizer state for continue of training.** If you want to save only weights or only a structure, follow the [link](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) to learn how to do that."
      ]
    },
    {
      "metadata": {
        "id": "1ioDJV9yUUHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('pushkin_gru_60.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KS02QoapWqao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we will define some functions for loading and uploading files to GDrive."
      ]
    },
    {
      "metadata": {
        "id": "JjIKIQTkUhvM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def upload_model_to_gdrive(model_name):\n",
        "    # Create & upload a file.\n",
        "    uploaded = drive.CreateFile({'title': model_name})\n",
        "    uploaded.SetContentFile(model_name)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "    \n",
        "def download_file_from_gdrive(file_name, id_file):\n",
        "    downloaded = drive.CreateFile({'id': id_file})\n",
        "    # fetch content\n",
        "    downloaded.FetchContent()\n",
        "    # save content to file on the disk\n",
        "    import shutil\n",
        "    downloaded.content.seek(0)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        shutil.copyfileobj(downloaded.content, f, length=131072)\n",
        "    print(\"File `{}` was successfully loaded\".format(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_drxTIzCnJNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c02d0318-c17e-465b-a188-e4cddb60081f"
      },
      "cell_type": "code",
      "source": [
        "download_file_from_gdrive('pushkin_gru.h5', '1CwqqC2TAoA7Jz4cF1mJIrEsZbjwzsaoO')\n",
        "model = load_model('pushkin_gru.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File `pushkin_gru.h5` was successfully loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p7hgfbzTVAl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e81b21ff-10c8-4fcc-d56b-e1eedf30ce76"
      },
      "cell_type": "code",
      "source": [
        "upload_model_to_gdrive('sampling.log')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1RhvlqTGwtDR6FIM-aBtD0UAGExACFD7Z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jHl3z1i8JFEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Useful Links\n",
        "\n",
        "\n",
        "\n",
        "*   [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)\n",
        "*   [Practical Deep Learning For Coders](http://course.fast.ai/)\n",
        "*   [open Machine Learning course](https://github.com/Yorko/mlcourse.ai)\n",
        "*   [Deep Learning Specialization (Coursera)](https://www.deeplearning.ai/)\n",
        "*   [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
        "*   [Speech and Language Processing (book)](https://web.stanford.edu/~jurafsky/slp3/)\n",
        "*   [Глубокое обучение. Погружение в мир нейронных сетей (book)](https://www.litres.ru/a-kadurin-13464223/glubokoe-obuchenie-pogruzhenie-v-mir-neyronnyh-29817855/)\n",
        "\n"
      ]
    }
  ]
}